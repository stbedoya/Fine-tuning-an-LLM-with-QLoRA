{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f79d1e-2e15-4976-aa2c-e77abd81c06f",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3-8B-Instruct with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21e0f8-78c3-47bd-8b54-22371ef144da",
   "metadata": {},
   "source": [
    "For this tutorial, we’ll fine-tune the Llama 3 8B-Instruct model using the ruslanmv/ai-medical-chatbot dataset. The dataset contains 250k dialogues between a patient and a doctor. QLoRA stands for Quantized Low-Rank Adapter, and it's a method introduced to fine-tune LLMs models using much less GPU memory - without sacrificing much performance.\n",
    "\n",
    "QLoRA combines two main ideas:\n",
    "1. Quantization (specifically 4-bit)\n",
    "    - It loads the base model weights in 4-bit precision instead of 16- or 32-bit.\n",
    "    - This saves massive amounts of VRAM (e.g., you can fine-tune LLaMA 13B on a single 24GB GPU).\n",
    "\n",
    "2. LoRA (Low-Rank Adaptation)\n",
    "    - Instead of updating all model weights, LoRA freezes them and adds small trainable \"adapter\" layers.\n",
    "    - These adapters inject learnable parameters into the model — they’re fast and cheap to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ccfa0c-60d8-495e-a98e-8630e5562edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df870e0-5152-41f7-a008-d23da7262b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.6.0+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/stefany/interview_Material/interviews/interview_env/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu11, nvidia-cuda-cupti-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, nvidia-cufft-cu11, nvidia-curand-cu11, nvidia-cusolver-cu11, nvidia-cusparse-cu11, nvidia-nccl-cu11, nvidia-nvtx-cu11, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, auto_gptq, bitsandbytes, peft, torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch # We need to verify that the installed versions of PyTorch and CUDA are compatible. It should display the version along with CUDA support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6233f9-79ee-4ff2-978f-e79d7f01a455",
   "metadata": {},
   "source": [
    "### Validate setup\n",
    "Let's start validating the setup. This ensures that the model, the GPU and libraries are configured correctly. Here’s what this script does:\n",
    "- Confirms that CUDA is available and functioning.\n",
    "- Verifies that the GPU is correctly detected.\n",
    "- Ensures the model and tokenizer load without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18047983-e14b-45e1-a704-e4eda5dc20b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefany/interview_Material/interviews/interview_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1061b68-28ef-4ad9-96fe-25dfa6093298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA RTX A5500 Laptop GPU\n",
      "QLoRA setup loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 3 loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Data type and attention implementation\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "print(\"QLoRA setup loaded successfully!\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "print(\"LLaMA 3 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d51e302-e5b6-4b0e-b195-ed67add358c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3198353c-6c5c-44a0-a90c-af79a48be592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af80aa2a-9275-4653-b5e4-7b817530ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Description': 'What causes blood in urine?', 'Patient': \"Dr, My daughter is 5yrs old.i saw stains in her trousers a few days ago.the stains were light red colour.later i found some pus like liquid near her urinary tract.yesterday i saw light brick coloured liquid along her urine.feeling panic,gave  urine for culter and routine test,culter result not yet recd.her routine test say,pus cells:4-8 and epithiall cells :2-4.What's wrong with my daughter? (she goes to urine only 4 to 5 times a day,drinking water too not sufficient)\", 'Doctor': 'Thanks for contacting HCMYou are concerned that your daughter may have a urinary tract infection. Your description of her urine and findings in her panties does suggest urinary tract infection. The urine analysis though is not very convincing for a urinary tract infection. The sample shows 2-4 epithelial cells and only 4-8 puss cells. The counts are normal and do not indicate infection. I recommend you wait for the culture results. I would recommend that your daughter drink plenty of fluids. I do not have an explaination for your daughters symptomsHope I answered your question. Please contact us again with your medical questions or concerns'}\n",
      "{'Description': 'What causes blood in urine?', 'Patient': \"Dr, My daughter is 5yrs old.i saw stains in her trousers a few days ago.the stains were light red colour.later i found some pus like liquid near her urinary tract.yesterday i saw light brick coloured liquid along her urine.feeling panic,gave  urine for culter and routine test,culter result not yet recd.her routine test say,pus cells:4-8 and epithiall cells :2-4.What's wrong with my daughter? (she goes to urine only 4 to 5 times a day,drinking water too not sufficient)\", 'Doctor': 'Thanks for contacting HCMYou are concerned that your daughter may have a urinary tract infection. Your description of her urine and findings in her panties does suggest urinary tract infection. The urine analysis though is not very convincing for a urinary tract infection. The sample shows 2-4 epithelial cells and only 4-8 puss cells. The counts are normal and do not indicate infection. I recommend you wait for the culture results. I would recommend that your daughter drink plenty of fluids. I do not have an explaination for your daughters symptomsHope I answered your question. Please contact us again with your medical questions or concerns', 'text': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nDr, My daughter is 5yrs old.i saw stains in her trousers a few days ago.the stains were light red colour.later i found some pus like liquid near her urinary tract.yesterday i saw light brick coloured liquid along her urine.feeling panic,gave  urine for culter and routine test,culter result not yet recd.her routine test say,pus cells:4-8 and epithiall cells :2-4.What's wrong with my daughter? (she goes to urine only 4 to 5 times a day,drinking water too not sufficient)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThanks for contacting HCMYou are concerned that your daughter may have a urinary tract infection. Your description of her urine and findings in her panties does suggest urinary tract infection. The urine analysis though is not very convincing for a urinary tract infection. The sample shows 2-4 epithelial cells and only 4-8 puss cells. The counts are normal and do not indicate infection. I recommend you wait for the culture results. I would recommend that your daughter drink plenty of fluids. I do not have an explaination for your daughters symptomsHope I answered your question. Please contact us again with your medical questions or concerns<|eot_id|>\"}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples\n",
    "print(dataset[1])  # Inspect the first sample\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(dataset[1]) # Inspect the format for chat template is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b2e88f-1e09-4d88-8cd2-d6bc47834d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Description', 'Patient', 'Doctor', 'text'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Description', 'Patient', 'Doctor', 'text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1) # Split dataset into training and validation set\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e259c192-6660-4303-b8f6-970414edc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"llama-3-8b-chat-doctor\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,  \n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    # report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f1cf309-69e8-4c9e-866c-9b15587d72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████████████████████████████████████████████| 900/900 [00:00<00:00, 22011.82 examples/s]\n",
      "Applying chat template to train dataset: 100%|█████████████████████████████████████████████| 900/900 [00:00<00:00, 37988.06 examples/s]\n",
      "Tokenizing train dataset: 100%|█████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 2468.91 examples/s]\n",
      "Truncating train dataset: 100%|███████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 123422.38 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|███████████████████████████████████████████████████| 100/100 [00:00<00:00, 15781.71 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████████████████████████████████████████| 100/100 [00:00<00:00, 20921.31 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2169.03 examples/s]\n",
      "Truncating eval dataset: 100%|█████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50129.13 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer, \n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5ce781-3773-4448-a5a8-9ca1444d4ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 13:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.251300</td>\n",
       "      <td>2.526964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.534300</td>\n",
       "      <td>2.475965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.286900</td>\n",
       "      <td>2.436862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.747200</td>\n",
       "      <td>2.415262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=2.5127355739805433, metrics={'train_runtime': 835.7442, 'train_samples_per_second': 1.077, 'train_steps_per_second': 0.538, 'total_flos': 9315430879100928.0, 'train_loss': 2.5127355739805433})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To clear out cache for unsuccessful run\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98466575-aaa5-4d11-ad41-5c50f2410ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
